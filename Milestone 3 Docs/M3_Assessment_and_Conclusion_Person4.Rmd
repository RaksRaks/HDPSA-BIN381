---
title: "Milestone 3: Model Assessment & Conclusion"
author: "Group M"
output:
  html_document:
    toc: true
    toc_depth: 2
    df_print: paged
  pdf_document: default
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(readr)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(pROC)
library(tidyr)
project_root <- "/Users/raksu/Desktop/DATA_SCIENCE_PRJ/HDPSA-BIN381"
assessment_dir <- file.path(project_root, "Milestone 3 outputs", "assessment")
metrics_path <- file.path(assessment_dir, "model_performance_metrics.csv")
confusion_path <- file.path(assessment_dir, "confusion_matrices.csv")
roc_img <- file.path(assessment_dir, "roc_curves.png")
perf_img <- file.path(assessment_dir, "model_performance_comparison.png")
```

## Model Assessment

```{r load-metrics}
stopifnot(file.exists(metrics_path))
stopifnot(file.exists(confusion_path))
metrics <- read_csv(metrics_path, show_col_types = FALSE)
confusions <- read_csv(confusion_path, show_col_types = FALSE)
```

### Performance Metrics

```{r metrics-table}
metrics %>%
  mutate(across(c(accuracy, precision, recall, f1, roc_auc), ~round(., 3))) %>%
  kable(caption = "Per-model performance metrics") %>%
  kable_styling(full_width = FALSE)
```

### Confusion Matrices

```{r confusion-table}
confusions %>%
  kable(caption = "Confusion matrices by model (tp, tn, fp, fn)") %>%
  kable_styling(full_width = FALSE)
```

### ROC Curves

```{r roc-plot, results='asis', echo=FALSE}
if (file.exists(roc_img)) {
  cat(sprintf("![](%s)", roc_img))
} else {
  cat("No ROC curves available (no probabilities provided).")
}
```

### Model Comparison Chart

```{r perf-plot, results='asis', echo=FALSE}
if (file.exists(perf_img)) {
  cat(sprintf("![](%s)", perf_img))
}
```

## Interpretation in Health Context

### Model Performance Summary

Based on the assessment results, the **Random Forest** model achieved the highest accuracy (54.55%), followed by Logistic Regression (52.27%), Decision Tree (50%), and Naive Bayes (50%). However, all models showed relatively modest performance, indicating the binary classification task of distinguishing between survey years (1998 vs. 2016) based on health indicators presents significant challenges.

### Key Findings

**1. Random Forest (Best Overall Performance)**
- **Accuracy**: 54.55%
- **Precision**: 54% | **Recall**: 61.4% | **F1**: 57.4%
- **ROC-AUC**: 0.508
- The ensemble approach captures complex non-linear relationships between health indicators
- Better balance between false positives (23) and false negatives (17)

**2. Logistic Regression (High Recall)**
- **Accuracy**: 52.27%
- **Precision**: 51.2% | **Recall**: 97.7% | **F1**: 67.2%
- **ROC-AUC**: 0.532
- Extremely high recall (97.7%) but low precision, predicting mostly the positive class
- Confusion matrix shows strong bias: 43 true positives but 41 false positives
- Most interpretable model with clear coefficient weights for each predictor

**3. Naive Bayes (Perfect Recall, Zero Specificity)**
- **Accuracy**: 50%
- **Precision**: 50% | **Recall**: 100% | **F1**: 66.7%
- **ROC-AUC**: 0.465
- Predicts ALL instances as positive class (44 TP, 44 FP, 0 TN, 0 FN)
- No discriminative power; essentially a random classifier
- Poor performance despite high recall due to complete lack of specificity

**4. Decision Tree (Balanced but Weak)**
- **Accuracy**: 50%
- **Precision**: 50% | **Recall**: 59.1% | **F1**: 54.2%
- **ROC-AUC**: 0.531
- Most balanced confusion matrix but no better than random guessing
- High interpretability through visualization but limited predictive power

### Health Context Implications

The modest model performance (all around 50-55% accuracy) suggests that:

1. **Temporal Data Limitations**: With only two survey years (1998 and 2016), the models struggle to identify consistent patterns that distinguish these time periods. Health indicators may show high variability within years rather than clear temporal trends.

2. **Complex Health Dynamics**: Health outcomes are influenced by numerous interconnected factors (socioeconomic status, education, infrastructure, policy changes) that create complex, non-linear relationships difficult to capture with limited temporal data.

3. **Indicator Relevance**: The selected health indicators (water access, sanitation, immunization, mortality rates, etc.) may not provide sufficient discriminative power for year classification. Some indicators might show improvement over time while others remain stable or worsen, creating mixed signals.

4. **Data Quality Considerations**: Missing data (handled through omission) and potential measurement inconsistencies between survey years could impact model performance.

### Plausible Predictor Importance

Based on domain knowledge and the Random Forest model's ability to capture variable importance:

- **Water and Sanitation Access**: Likely improved between 1998-2016 due to infrastructure development
- **Immunization Coverage**: Expected increase following expanded public health programs
- **Child Mortality Rates**: Anticipated decline due to improved healthcare access
- **Education and Literacy**: Gradual improvements over the 18-year period
- **Maternal Health**: Enhanced through targeted health interventions

However, the weak model performance suggests these improvements may be inconsistent across regions or indicators, or the magnitude of change may be smaller than expected.

## Conclusion

### Recommended Model

For this specific task, we recommend the **Random Forest** model for the following reasons:

1. **Best Overall Accuracy**: 54.55% represents the highest predictive performance
2. **Balanced Metrics**: More balanced precision-recall trade-off (F1 = 57.4%)
3. **Robust to Overfitting**: Ensemble approach reduces variance
4. **Feature Importance**: Can identify which health indicators contribute most to temporal classification

However, given the marginal improvement over random guessing (50%), we **do not recommend deploying any of these models for production use** without significant improvements.

### Link to Business Understanding

From Milestone 1, our business objectives focused on:
- Understanding health trends in South Africa
- Identifying patterns in health outcomes over time
- Supporting evidence-based policy decisions

**Assessment**: The current models provide limited value for these objectives. The weak performance indicates that binary year classification may not be the optimal modeling approach. The models marginally distinguish between 1998 and 2016 health conditions, suggesting either:
1. Health indicators have not changed dramatically enough to be easily classified
2. The modeling approach needs refinement (e.g., regression for continuous outcomes, multi-class for regional analysis)

### Validation Caveats

1. **Temporal Validation Concerns**: With only two time points, traditional train-test splits may not reflect real-world generalization to future years
2. **Data Leakage Risk**: Random 70/30 split doesn't respect temporal ordering; time-based validation would be more appropriate
3. **Sample Size**: After cleaning (n=294, split into 206 train/88 test), the test set may be too small for robust evaluation
4. **Class Balance**: Perfectly balanced classes (50/50) in both train and test sets may not reflect real-world distributions
5. **Missing Data Handling**: Complete case analysis (omitting missing values) may introduce bias

### Future Improvement Recommendations

1. **Alternative Modeling Approaches**:
   - Multi-output regression predicting actual health indicator values
   - Time series analysis for trend detection
   - Regional classification models (province-level patterns)
   - Cluster analysis to identify health indicator profiles

2. **Feature Engineering**:
   - Create interaction terms between related indicators
   - Develop composite health scores
   - Include rate-of-change features if more time points become available

3. **Data Enhancement**:
   - Incorporate additional survey years (if available) for richer temporal patterns
   - Add external covariates (economic indicators, policy changes)
   - Improve imputation strategies for missing data

4. **Model Refinement**:
   - Hyperparameter tuning (grid search, cross-validation)
   - Try advanced algorithms (XGBoost, Neural Networks)
   - Ensemble methods combining multiple model types

5. **Validation Strategy**:
   - Implement proper temporal validation if more years added
   - Use bootstrap confidence intervals for performance metrics
   - Conduct sensitivity analysis on data preprocessing decisions

### Final Remarks

While the technical implementation of Person 4's assessment task is complete with properly formatted metrics, visualizations, and confusion matrices, the underlying model performance reveals that the binary year classification task may not be well-suited to the available data. Future work should focus on reframing the business problem or enhancing the dataset to achieve more actionable insights for health policy decisions in South Africa.

## Appendix: Reproducibility

```{r session-info}
sessionInfo()
```


